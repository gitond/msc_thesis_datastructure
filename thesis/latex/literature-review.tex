\chapter{Literature review} \label{Literature review}

\section{Client-Server Architectures    \}    \}} \label{csa}

\section{Computer Vision (CV)           \}    \}    (COMBINE IF NEEDED)} \label{cv}

\section{Augmented Reality (AR)               \}} \label{ar}

\section{Prototypes Similar to Ours} \label{protos}
Before designing and describing our own software architecture it is 
worthwhile to look at applications other people have developed that solve a 
problem similar to ours. A great example of such an application is the work 
done by \textcite{pylvanainen}. \par
	The work conducted by \textcite{pylvanainen} started from a simple
observation: They coulnd't find any mobile apps for microscopy education that 
incorporated AR and Virtual Reality (VR) features and step-by-step guidance. 
They then sent out a needs assesment survey to students to map out demand for 
such a software and found that 70\% of the respondents showed interest in 
using such an app in their microscopy studies.\cite{pylvanainen} They then 
outlined goals for such an app and started to develop it. 
\textcite{pylvanainen} state that their application should:
\begin{enumerate}
	\item Be a useful tool in teaching microscopy
	\item Help its users to operate a microscope
	\item Be a helpful tool with troubleshooting microscopy related issues
	\item Be a tool that could be used to revive microscopy knowledge after a long pause in practicing microscopy skills
\end{enumerate} \par
	The app developed by \textcite{pylvanainen} consists of three 
sections: "Teach me microscopy", "Help me at the microscope" and "Help me to 
troubleshoot". What's relevant to this thesis is under "Help me at the 
microscope". In that section there is the option to view a 3D-model of a 
specific microscope commonly used in laboratories (Leica DM RXA microscope). 
Interactive step-by-step tutorials are also available for this microscope on 
various things, such as microscopy parts, setting optimal KÃ¶hler alignment 
and focusing the microscope on the sample. These tutorials are also usable 
outside the virtual microscope in the real world as the "Help me at the 
microscope"-section also acts as a marker-based 
AR-environment.\cite{pylvanainen} AR-markers put on the microscopes can also 
help students find different parts of the microscope and learn of their 
functions. This system can also be used to integrate microscope-specific 
information into this AR-environment.\cite{pylvanainen} \par
	\textcite{pylvanainen} also conducted a questionnaire-based usability 
study on the app they developed, and found that using the app during 
microscopy education increased the students' confidence at later using the 
same microscope independently, without assistance. Furthermore 64\% of the 
students reported that the app definitely helped them at learning microscopy 
and 90\% of them reported that the app helped them recall microscopy skills 
later.\cite{pylvanainen} \par
	Another quite similar project is the work carried out by 
\textcite{estrada}. Their work was built on a simple goal: To enable students 
to have a better experience when learning how to use electrical engineering 
laboratory equipment. They aimed to do this by offering the students AR-based 
tutorials for various electrical engineering lab equipment, and by using Deep 
Learning (DL) methods to detect such equipment in the 
laboratory.\cite{estrada} The long term goal of this project was to create a 
framework that could be later used to easily develop interactive smartphone 
apps for different laboratory devices integrating this 
concept.\cite{estrada} \par
	Essentially \textcite{estrada} developed a superimposition-based 
AR-app with an integrated DL-model to be used for object detection. This 
application can be used for a template to create any AR-based tutorial for 
any device. Creating a tutorial for a device using this framework consists of 
three steps of work:
\begin{enumerate}
	\item Training the DL-model to recognize the device
	\item Creating an Augmented Reality User Interface (AR-UI) of the device, consisting of a 3D-model and User Interface (UI) panels
	\item Creating step-by-step instructions that can be displayed using the AR-UI defined in the first step.
\end{enumerate} \par
	On top of designing and defining such a framework, \textcite{estrada} 
actually developed an application using it. In this application the DL-model 
was trained to detect various types of multimeters, oscilloscopes, wave 
generators and power supplies. They also created AR-UIs and step-by-step 
tutorials for using real multimeters.\cite{estrada}\par
	The application logic of the app by \textcite{estrada} is as follows: 
First the DL-model detects (so classifies and localizes) the equipment. Next 
if a tutorial is available for that equipment, the UI notifies the user of this.
If the user decides to view the tutorial, the AR-based tutorial gets loaded 
and the AR-UI gets superimposed on top of the real object. Then UI-panels are 
used to display the tutorial content. \par
\\
\\
\\
Stepping outside of the laboratory setting, \textcite{VanGestel2024} developed 
an AR-application for helping orthopedic procedures. Namely they wanted to 
create a tool that would act as a new real-time AR-based safety solution and 
guidance technique in the use of power tools in surgery. Prior to this there 
did exist other camera and AR-based surgery systems but 
\textcite{VanGestel2024} noted them to be physically too large, expensive and 
time-consuming, which was said to limit their usefulness in assisting with 
performing surgeries. \textcite{VanGestel2024} wanted to develop a solution 
that could run on a head-mounted display (HMD) so that a surgeon could use 
it while working with his/her hands. However one problem was that HMDs 
typically couldn't do accurate enough tracking for surgical 
use.\cite{VanGestel2024}\par
	\textcite{VanGestel2024} don't describe the structure and logic 
of the software built for their task very deeply. They do mention building it 
for Microsoft's HoloLens headset, and circumventing the poor performance of 
its camera's tracking ability by using the built in infrared sensor 
instead.\cite{VanGestel2024} They measured the infrared sensor's tracking 
accuracy to be below 1mm, accurate enough for surgical 
work.\cite{VanGestel2024} Their software technically uses marker-based AR, 
however it must be noted that the markers they use are not physical markers, 
rather markers registered by an infrared-tracked stylus at the key positions 
to the surgical operation.\cite{VanGestel2024} These virtual markers were then 
used to show the users an AR-based guidance vector representing the desired 
drilling direction when performing the surgery. The vector would also change 
color to represent whether the current drilling direction was correct or not. 
If the surgeon was drilling in the right direction the vector would be green, 
otherwise the vector would be orange or red.\cite{VanGestel2024}\par
	This software was then tested by letting 18 people perform mock 
surgeries on wooden models.\cite{VanGestel2024} Three surgery guidance 
techniques were compared: freehand surgery without guidance, 
proprioception-guided surgery and surgery using the new 
AR-tool.\cite{VanGestel2024} The mock surgeries were quite simple: the wooden 
bone-models had defined entry and exit points between which the surgeons had 
to drill, with parts of the models being covered by a cloth to better 
simulate real conditions.\cite{VanGestel2024}
