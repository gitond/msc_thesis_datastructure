@article{pylvanainen,
	title={Supporting Microscopy Learning with Ocul-AR, a Virtual and Augmented Reality-Powered Mobile Application},
	author={Joanna Pylvänäinen and Junel Solis and Diana Toivola and Pasi Kankaanpää},
	year={2023},
	month={may},
	volume={3393},
	pages={28-37},
	journal={CEUR Workshop Proceedings},
	issn={1613-0073},
	publisher={CEUR Workshop Proceedings},
	note={Technology-Enhanced Learning in Laboratories workshop (TELL) ; Conference date: 27-04-2023}
}

@Article{estrada,
	author={Estrada, John and Paheding, Sidike and Yang, Xiaoli and Niyaz, Quamar},
	title={Deep-Learning-Incorporated Augmented Reality Application for Engineering Lab Training},
	journal={Applied Sciences},
	volume={12},
	year={2022},
	number={10},
	url={https://www.mdpi.com/2076-3417/12/10/5159},
	issn={2076-3417},
	doi={10.3390/app12105159}
}

@Article{VanGestel2024,
	author={Van Gestel, Frederick and Van Aerschot, Fiene and Frantz, Taylor and Verhellen, Anouk and Barb, Kurt and Jansen, Bart and Vandemeulebroucke, Jef and Duerinck, Johnny and Scheerlinck, Thierry},
	title={Augmented reality guidance improves accuracy of orthopedic drilling procedures},
	journal={Scientific Reports},
	year={2024},
	month={Oct},
	day={25},
	volume={14},
	number={1},
	pages={25269},
	issn={2045-2322},
	doi={10.1038/s41598-024-76132-3},
	url={https://www.nature.com/articles/s41598-024-76132-3#citeas}
}

@article{reyesEtAl2016,
	author={Monroy Reyes, Alejandro and Vergara Villegas, Osslan Osiris and Miranda Bojórquez, Erasmo and Cruz Sánchez, Vianey Guadalupe and Nandayapa, Manuel},
	title={A mobile augmented reality system to support machinery operations in scholar environments},
	journal={Computer Applications in Engineering Education},
	volume={24},
	number={6},
	pages={967-981},
	keywords={mobile augmented reality, manufacturing environments, machinery operation, technology acceptance, lathe and milling},
	doi={https://doi.org/10.1002/cae.21772},
	url={https://onlinelibrary.wiley.com/doi/abs/10.1002/cae.21772},
	eprint={https://onlinelibrary.wiley.com/doi/pdf/10.1002/cae.21772},
	year={2016}
}

@InProceedings{LinAndLee2020,
	author={Lin, Yu-Ting and Lee, I-Jui},
	editor={Chen, Jessie Y. C. and Fragomeni, Gino},
	title={Development of an Augmented Reality System Achieving in CNC Machine Operation Simulations in Furniture Trial Teaching Course},
	booktitle={Virtual, Augmented and Mixed Reality. Industrial and Everyday Life Applications},
	year={2020},
	publisher={Springer International Publishing},
	address={Cham},
	pages={121-135},
	isbn={978-3-030-49698-2}
}

@article{ghasemi,
	title = {Deep learning-based object detection in augmented reality: A systematic review},
	journal = {Computers in Industry},
	volume = {139},
	pages = {103661},
	year = {2022},
	issn = {0166-3615},
	doi = {https://doi.org/10.1016/j.compind.2022.103661},
	url = {https://www.sciencedirect.com/science/article/pii/S0166361522000586},
	author = {Yalda Ghasemi and Heejin Jeong and Sung Ho Choi and Kyeong-Beom Park and Jae Yeol Lee},
}

@misc{minaee2022modernaugmentedrealityapplications,
	title={Modern Augmented Reality: Applications, Trends, and Future Directions}, 
	author={Shervin Minaee and Xiaodan Liang and Shuicheng Yan},
	year={2022},
	eprint={2202.09450},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2202.09450}, 
}

@InProceedings{locAR,
	author={Batuwanthudawa, B.I. and Jayasena, K.P.N},
	booktitle={2020 2nd International Conference on Advancements in Computing (ICAC)}, 
	title={Real- Time Location based Augmented Reality Advertising Platform}, 
	year={2020},
	volume={1},
	pages={174-179},
	keywords={Navigation;Education;Cameras;Real-time systems;Advertising;Augmented reality;Business;Augmented Reality;Advertising Platform;Location based AR;Unity Engine},
	doi={10.1109/ICAC51239.2020.9357261},
}

@article{slam,
	author={Bailey, T. and Durrant-Whyte, H.},
	journal={IEEE Robotics & Automation Magazine}, 
	title={Simultaneous localization and mapping (SLAM): part II}, 
	year={2006},
	volume={13},
	number={3},
	pages={108-117},
	keywords={Simultaneous localization and mapping;Vehicles;Computational complexity;Computational efficiency;Delay estimation;Uncertainty;Robotics and automation;Mobile robots;Robustness;Bayesian methods},
	doi={10.1109/MRA.2006.1678144}
}

@inproceedings{lamp,
	author = {Kim, Jeongyun and Seo, Jonghoon and Han, Tack-Don},
	title = {AR Lamp: interactions on projection-based augmented reality for interactive learning},
	year = {2014},
	isbn = {9781450321846},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2557500.2557505},
	doi = {10.1145/2557500.2557505},
	abstract = {Today, people use a computer almost everywhere. At the same time, they still do their work in the old-fashioned way, such as using a pen and paper. A pen is often used in many fields because it is easy to use and familiar. On the other hand, however, it is a quite inconvenient because the information printed on paper is static. If digital features are added to this paper environment, the users can do their work more easily and efficiently. AR (augmented reality) Lamp is a stand-type projector and camera embedded system with the form factor of a desk lamp. Its users can modify the virtually augmented content on top of the paper with seamlessly combined virtual and physical worlds. AR is quite appealing, but it is difficult to popularize due to the lack of interaction. In this paper, the interaction methods that people can use easily and intuitively are focused on. A high-fidelity prototype of the system is presented, and a set of novel interactions is demonstrated. A pilot evaluation of the system is also reported to explore its usage possibility.},
	booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
	pages = {353–358},
	numpages = {6},
	keywords = {projection-based augmented reality, pen computing, finger gesture, bimanual interaction},
	location = {Haifa, Israel},
	series = {IUI '14}
}

@book{klette2014hv,
	title = {Concise computer vision},
	author = {Klette, Reinhard},
	publisher = {Springer},
	series = {Undergraduate Topics in Computer Science},
	edition = {2014},
	month = {jan},
	year = {2014},
	address = {Guildford, England},
	language = {en}
}

@inproceedings{zhangFronzNavab,
	author = {Xiang Zhang and Fronz, S. and Navab, N.},
	booktitle = {Proceedings. International Symposium on Mixed and Augmented Reality}, 
	title = {Visual marker detection and decoding in AR systems: a comparative study}, 
	year = {2002},
	pages = {97-106},
	keywords = {Decoding;Augmented reality;Tracking;Calibration;Computer aided manufacturing;Military computing;Computer displays;Cameras;Thyristors;Performance evaluation},
	doi = {10.1109/ISMAR.2002.1115078}
}

@misc{opencvHarris,
	author = {},
	title = {OpenCV: Harris Corner Detection --- docs.opencv.org},
	howpublished = {\url{https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html}},
	year = {},
	note = {[Accessed 27-05-2025]},
}

@misc{liuziwei7Deepfashion1Poster, 
	title = {DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},
	url = {https://liuziwei7.github.io/projects/DeepFashion.html}, 
	journal = {DeepFashion: Powering robust clothes recognition and retrieval with rich annotations}, 
	author = {Liu, Ziwei and Luo, Ping and Qiu, Shi and Wang, Xiaogang and Tang, Xiaoou}, 
	year = {2016}
} 

@article{hanVitonArxiv,
	author = {Xintong Han and Zuxuan Wu and Zhe Wu and Ruichi Yu and Larry S. Davis},
	title = {VITON: An Image-based Virtual Try-on Network},
	journal = {CoRR},
	volume = {abs/1711.08447},
	year = {2017},
	url = {http://arxiv.org/abs/1711.08447},
	eprinttype = {arXiv},
	eprint = {1711.08447},
	timestamp = {Mon, 09 Dec 2019 13:47:05 +0100},
	biburl = {https://dblp.org/rec/journals/corr/abs-1711-08447.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liuMakeupArxiv,
	author = {Si Liu and Xinyu Ou and Ruihe Qian and Wei Wang and Xiaochun Cao},
	title = {Makeup like a superstar: Deep Localized Makeup Transfer Network},
	journal = {CoRR},
	volume = {abs/1604.07102},
	year = {2016},
	url = {http://arxiv.org/abs/1604.07102},
	eprinttype = {arXiv},
	eprint = {1604.07102},
	timestamp = {Thu, 07 Nov 2019 09:05:08 +0100},
	biburl = {https://dblp.org/rec/journals/corr/LiuOQWC16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{lin2015microsoftcococommonobjects,
	title={Microsoft COCO: Common Objects in Context}, 
	author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
	year={2015},
	eprint={1405.0312},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1405.0312}
}

@article{moore2020fiftyone,
	title={FiftyOne},
	author={Moore, B. E. and Corso, J. J.},
	journal={GitHub. Note: https://github.com/voxel51/fiftyone},
	year={2020}
}

@misc{tensorflow2015-whitepaper,
	title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={https://www.tensorflow.org/},
	note={Software available from tensorflow.org},
	author={Mart\'{i}n Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dandelion Man\'{e} and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Vi\'{e}gas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
	year={2015}
}

@misc{tfobdTutorial,
	title={Object detection with model garden : Tensorflow Core},
	url={https://www.tensorflow.org/tfmodels/vision/object_detection},
	journal={TensorFlow},
	author={Patlolla, Laxma Reddy and Neeli, Siva Sravana Kumar and Daoust, Mark and bharatjetti},
	year={2025},
	month={Feb}
} 

﻿@Article{sedaghati2025,
	author={Sedaghati, Nazanin and Ardebili, Sondos and Ghaffari, Ali},
	title={Application of human activity/action recognition: a review},
	journal={Multimedia Tools and Applications},
	year={2025},
	month={Aug},
	day={01},
	volume={84},
	number={28},
	pages={33475-33504}, 
	abstract={Human activity recognition is a crucial domain in computer science and artificial intelligence that involves the Detection, Classification, and Prediction of human activities using sensor data such as accelerometers, gyroscopes, etc. This field utilizes time-series signals from sensors present in smartphones and wearable devices to extract human activities. Various types of sensors, including inertial HAR sensors, physiological sensors, location sensors, cameras, and temporal sensors, are employed in diverse environments within this domain. It finds valuable applications in various areas such as smart homes, elderly care, the Internet of Things (IoT), personal care, social sciences, rehabilitation engineering, fitness, and more. With the advancement of computational power, deep learning algorithms have been recognized as effective and efficient methods for detecting and solving well-established HAR issues. In this research, a review of various deep learning algorithms is presented with a focus on distinguishing between two key aspects: activity and action. Action refers to specific, short-term movements and behaviors, while activity refers to a set of related, continuous affairs over time. The reviewed articles are categorized based on the type of algorithms and applications, specifically sensor-based and vision-based. The total number of reviewed articles in this research is 80 sources, categorized into 42 references. By offering a detailed classification of relevant articles, this comprehensive review delves into the analysis and scrutiny of the scientific community in the HAR domain using deep learning algorithms. It serves as a valuable guide for researchers and enthusiasts to gain a better understanding of the advancements and challenges within this field.},
	issn={1573-7721},
	doi={10.1007/s11042-024-20576-2},
	url={https://doi.org/10.1007/s11042-024-20576-2}
}
