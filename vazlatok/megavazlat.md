## 3 Literature review

### 3.1 Client-Server Architectures

### 3.2 Computer Vision (CV)
> "Deep learning (DL), a sub-field of machine learning (ML), embraces artificial neural networks (ANN), which are algorithms inspired by the structure and function of the human brain"
(Estrada et al., 2022)

> "Object detection is primarily used in computer vision and has gained popularity in a variety of applications over the last decade, including autonomous vehicles, intelligent video, and surveillance systems<br/>&emsp;For object detection, a type of deep neural network called a convolutional neural network (CNN) [19] has been widely used."
(Estrada et al., 2022)

> "In recent years, object detection methods such as the region-based convolutional neural network (RCNN) [22], you only look once (YOLO) [23], and single shot detector (SSD) [24] have been proposed."
(Estrada et al., 2022)

### 3.3 Augmented Reality (AR)
> "AR superimposes three-dimensional objects on the physical world, requiring the use of mobile devices to create interactions. MR is a technology that combines the physical and digital worlds to create immersive physical experiences. Users interact with both the physical and digital worlds by using their five senses."
(Estrada et al., 2022)

> "There are diverse types of AR suitable for different applications despite the fact that they all have similar capabilities [32,33]. Figure 1 depicts the two primary types of AR: marker-based AR and marker-less AR"
(Estrada et al., 2022)

> "Marker-based AR works when it is triggered by pre-defined markers. It allows the user to choose where to place the virtual object. Barcodes and QR codes are commonly used as images or photo symbols to be placed on flat surfaces. The program recognizes the marker when the mobile device focuses the target image."
(Estrada et al., 2022)

> "Marker-less AR collects data from the device hardware such as a camera, a GPS, a digital compass, and an accelerometer for the AR program to function. Marker-less AR applications rely on computer vision algorithms to distinguish objects, and they can function in the real world without specific markers [37,38]."
(Estrada et al., 2022)

> "Location-based AR: In this type of AR, simultaneous localization and mapping (SLAM) technology is used to track the user’s location as the map is generated and updated on the user’s mobile device"
(Estrada et al., 2022)

> "Superimposition-based AR: Superimposition-based AR applications can provide an additional view along with the original view of the object. Object recognition is required to determine the type of object to partially or completely replace an object in the user’s environment with a digital image [43,44]."
(Estrada et al., 2022)

> "Projection-based AR: Projection-based AR (also known as projection mapping and augmented spatial reality) is a technique that does not require the use of head-mounted or hand-held devices. This method allows augmented information to be viewed immediately from a natural perspective. Using projection mapping, projection-based AR turns an uneven surface into a projection screen."
(Estrada et al., 2022)

> "Vuforia’s Model Target is an example of outlining-based AR. Vuforia is a platform that enables developers to quickly incorporate AR technology into their applications. Model Targets allow apps to recognize and track real-world objects based on their shape"
(Estrada et al., 2022)

### 3.4 Prototypes Similar to Ours
> "we developed a mobile application, Ocul-AR, for microscopy teaching and support" 
(Pylvänäinen et al., 2023)

> "Ocul-AR was designed by a multidisciplinary team to guide microscopy students and users to learn about microscopy, optimize light paths and operate light microscopes independently"
(Pylvänäinen et al., 2023)

> "All respondents (n=11) reported that Ocul-AR helped them to learn hands-on microscopy (64% replied that the application helped definitely, 36% that it helped somewhat), as well as helped them to recall microscopy skills (90% definitely, 10% somewhat). Students who used Ocul-AR during the course felt they gained confidence to operate the microscope during the hands-on session (82% definitely, 18% somewhat) and that it lowered their threshold for using the microscope independently (70% definitely, 30% somewhat)."
(Pylvänäinen et al., 2023)

> "To our knowledge, there are no AR/VR-based mobile applications for higher education that teach microscopy with a step-by-step approach. <br/>&emsp;To facilitate the development of researchers' and students' proficiency in using microscopes easily and cost-effectively, we developed a mobile application called Ocul-AR. The main aims of Ocul-AR are to 1) teach students about microscopy, 2) aid during microscope operation, 3) help in troubleshooting unexpected issues, and 4) help refresh skills after extended periods of not using a microscope"
(Pylvänäinen et al., 2023)

> "Here we show that students who used Ocul-AR during a microscopy course gained more confidence to operate the microscope and that Ocul-AR lowered their threshold for using the microscope independently"
(Pylvänäinen et al., 2023)

> "a Needs assessment survey was sent by email to the students in order to investigate the biggest challenges when trying to use a microscope independently"
(Pylvänäinen et al., 2023)

> "All respondents showed interest in using a mobile application for learning microscopy, 70% very much and 30% maybe. Students were interested in a feature that would allow them to track their progress during learning and said that getting rewards would inspire them to continue learning by using the application."
(Pylvänäinen et al., 2023)

> "At Level 1, there are three categories: Teach me microscopy, Help me at the microscope and Help me to troubleshoot (Figure 2, Figure 3A)."
(Pylvänäinen et al., 2023)

> "The first level 1 category, Teach me microscopy, is intended mainly for independent studies before or during microscopy sessions. This section is further divided into three sections (Level 2); Virtual microscope, Test your knowledge and Learn more. The Virtual microscope contains a 3D model of a typical brightfield microscope, the Leica DM RXA microscope (Leica Microsystems), with an interactive tutorial about microscopy parts, how to set optimal Köhler alignment and how to focus on the sample (Figure 3B). This feature can also be used as a step-by-step guide while at this specific microscope."
(Pylvänäinen et al., 2023)

> "After pressing Help me at the microscope, the application opens an AR environment, where it is possible to scan markers or QR codes that have been attached to the microscope beforehand (Figure 3C). These markers guide the student in finding different parts of the microscope and their functions and switching the microscope on and off."
(Pylvänäinen et al., 2023)

> "As a result of the VR and AR features, students could engage with the microscope and access microscope-specific information from their homes or while physically using the equipment"
(Pylvänäinen et al., 2023)

> "While practical hands-on training cannot be entirely replaced, our solution can supplement it by providing additional support."
(Pylvänäinen et al., 2023)

> "Deep learning (DL) algorithms have achieved significantly high performance in object detection tasks. At the same time, augmented reality (AR) techniques are transforming the ways that we work and connect with people. With the increasing popularity of online and hybrid learning, we propose a new framework for improving students’ learning experiences with electrical engineering lab equipment by incorporating the abovementioned technologies."
(Estrada et al., 2022)

> "The DL powered automatic object detection component integrated into the AR application is designed to recognize equipment such as multimeter, oscilloscope, wave generator, and power supply."
(Estrada et al., 2022)

> "When a piece of equipment is detected, the corresponding AR-based tutorial will be displayed on the screen"
(Estrada et al., 2022)

> "Furthermore, to demonstrate practical application of the proposed framework, we develop a multimeter tutorial where virtual models are superimposed on real multimeters"
(Estrada et al., 2022)

> "This work explores the idea of using equipment recognition and an AR-based tutorial to enhance student learning experiences with electrical equipment in their engineering laboratories. Our long-term goal is to develop interactive smartphones apps for lab equipment such as multimeters, oscilloscopes, wave generators, and power supplies. Object detection using DL methods fits our goal because the app can detect specific electrical equipment in the lab with high precision in real-time using state-of-art DL algorithms"
(Estrada et al., 2022)

> "In this work, we are using a deep neural network to detect objects"
(Estrada et al., 2022)

> "In this paper, we compared RCNN and MobileNet-SSD v2 [26] and observed that MobileNet-SSD v2 has better performance for real-time applications in terms of speed when implemented on mobile devices. It is important to note that MobileNet-SSD v2 is a lightweight deep neural network architecture designed specifically for mobile devices with high recognition accuracy. Therefore, we employed MobileNet-SSD v2 in this work."
(Estrada et al., 2022)

> "In our project, we built a superimposition-based AR app. We built user interfaces on top of lab equipment, allowing step-by-step instructions to be incorporated into the application for users to understand and learn how to use specific equipment."
(Estrada et al., 2022)

> "The inference will classify and localize lab equipment that has been targeted with the mobile camera. When an object is detected, a user interface (UI) button appears, indicating that an AR-guided tutorial is available for the object. Then, an AR scenario will be loaded, allowing students to use their mobile camera to aim at a specific target. Following that, a 3D object will superimpose on top of the physical object, activating UI panels with instructions on how to use the equipment"
(Estrada et al., 2022)

## 4 Architecture Description

### 4.1 Perceived Challenges

### (4.1.5 Technical details of prototypes)
This could be displayed as a table for example

> "The source code for the application was written in the Unity 2020.3.3 real-time development platform using the C# programming language, resulting in a mobile application based around a singleton class that manages various application states"
(Pylvänäinen et al., 2023)

> "The AR feature was implemented using EasyAR Sense 4.4. Quick response (QR) code markers placed on different parts of the microscope allow the user to scan and retrieve information about specific parts"
(Pylvänäinen et al., 2023)

> "The application was designed to run on the Android mobile operating system and was distributed to end-user devices via Android Package (APK) files."
(Pylvänäinen et al., 2023)

> "A deep neural network model, namely MobileNet-SSD v2, is implemented for equipment detection using TensorFlow’s object detection API"
(Estrada et al., 2022)

> "The Unity3D game engine is used as the primary development tool for this tutorial to integrate DL and AR frameworks and create immersive scenarios"
(Estrada et al., 2022)

> "Unity 3D combines the output of these systems by inferring the object detection model with OpenCV and using an AR dataset target with a Vuforia Engine. Furthermore, Unity 3D enables the development of interactive user interfaces"
(Estrada et al., 2022)

<img src="https://www.mdpi.com/applsci/applsci-12-05159/article_deploy/html/images/applsci-12-05159-g002-550.jpg">
(Estrada et al., 2022)

> "The development framework was integrated with a MobileNet-SSD DL model and a marker-less superimposition AR that activates immersive modules containing 2D/3D objects"
(Estrada et al., 2022)

> "MobileNet-SSDv2 [26] architecture was used to build a deep neural network model to detect electrical lab equipment. The architecture comprised MobileNet-v2 as the backbone network, an SSD detector, and feature pyramid network (FPN)"
(Estrada et al., 2022)

## 6 Usability

### (6.1 Usability in prototypes)
> "All the participants indicated that the Ocul-AR application was aesthetically pleasing and simple to navigate. The participants noted that the application was appropriate for novice users but suggested that it should include more advanced content if intended for advanced users."
(Pylvänäinen et al., 2023)

<img src="https://cdn.discordapp.com/attachments/524629394208325642/1346074587619655690/image.png?ex=67c6dd26&is=67c58ba6&hm=562b16e90f7f502e7d35cf26b983db6e1e71e13dad0105470179cd66ffd5b130&">
(Pylvänäinen et al., 2023)

> "Following their initial independent use of the microscope supported by the Ocul-AR application, all participants reported that the application helped facilitate their understanding of microscopy. Specifically, 64% of participants reported that the application was definitely helpful, while 36% reported that it was somewhat helpful."
(Pylvänäinen et al., 2023)

> "3.4. Phase 2 results: Self-study with Ocul-AR application by the microscope helps students to follow the conventional microscopy hands-on training"
(Pylvänäinen et al., 2023)

> "Pilot study phase 3 was conducted to evaluate the long-term effectiveness of Ocul-AR in learning microscopy. During this phase, students were requested to perform the same tasks as in phase 1, but after a gap of three months following the guided hands-on session of the course"
(Pylvänäinen et al., 2023)

> "As the application was used for these specific tasks during teaching, the students reported that it was a familiar resource for revising how to perform these tasks. 64 % of the students felt that the Ocul-AR application definitely helped them in becoming independent microscopy users, whereas 27 % reported that the application helped somewhat. 9 % of the students felt that Ocul-AR did not significantly help them in becoming independent microscopy users"
(Pylvänäinen et al., 2023)
